# üñºÔ∏è Image Classification with CIFAR-10 using Transfer Learning (ResNet50)

## üìå Project Overview

This project focuses on building a deep learning-based **image classifier** for the CIFAR-10 dataset using **transfer learning** with **ResNet50** pre-trained model.  
We start with a **baseline model** (31.76% accuracy) and progressively fine-tune it using:
- Unfreezing ResNet50 layers
- Adding custom dense layers with dropout
- Lowering the learning rate
- Applying data augmentation
- Using early stopping to prevent overfitting

The final model achieved **88.82% accuracy** on the test set.

---

## üéØ Goal

To develop a **highly accurate, generalizable** image classification model by leveraging **transfer learning** and advanced training techniques, making it applicable to real-world scenarios such as product categorization, surveillance, and autonomous systems.

---

## üíº Business Context

Image classification is critical in:
- **Retail** (automatic product tagging)
- **Automotive** (object detection in self-driving cars)
- **Healthcare** (medical imaging classification)
- **Security** (identifying threats in real time)

By applying deep learning with transfer learning, companies can leverage pre-trained models on massive datasets and adapt them to their own classification tasks without starting from scratch ‚Äî saving both time and computational resources.

---

## üåç Real-World Applications

- **E-commerce**: Categorizing products from images automatically.
- **Autonomous Vehicles**: Detecting traffic objects and hazards.
- **Medical Imaging**: Classifying medical scans for diagnostics.
- **Social Media**: Filtering inappropriate image content.

---

## üìÇ Dataset

The project uses the **CIFAR-10** dataset:
This **dataset is also available** in TensorFlow & Keras. 
Here is how you can import it: from tensorflow.keras.datasets import cifar10
- **60,000** 32x32 RGB images
- **10 classes**, each with **6,000 images**
- **50,000 training** images and **10,000 test** images

**Classes:** airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.

---

## üìä Project Workflow

1. **EDA Summary**
   
   **- Dataset shape & class distribution**
     
       ‚Ä¢    Training set: 10,000 RGB images, size 32√ó32√ó3
     
	   ‚Ä¢	Test set: 10,000 RGB images, size 32√ó32√ó3

	   ‚Ä¢	Classes: 10 mutually exclusive categories (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)

       ‚Ä¢	Balanced across categories, ranging from 937 to 1,032 images per class.
       
   **- Sample image visualization**
   **- Pixel intensity analysis**
     
     	‚Ä¢	Most pixel values range from 50 to 150 (moderate brightness).
     
	    ‚Ä¢	Few pixels are near 0 (pure black) or 255 (pure white).
     
	    ‚Ä¢	Indicates well-lit, balanced images without extreme contrast.
     
        ‚Ä¢	Red & Green channels peak around 100‚Äì120, fairly symmetric.
     
	    ‚Ä¢	Blue channel slightly darker overall, peaking near 100.
     
	    ‚Ä¢	No strong color bias ‚Äî suitable for CNN training.
     
   **- Data Quality and Basic Statistics**
     
       ‚Ä¢	No missing, NaN, or Inf values.
     
	   ‚Ä¢	All images have consistent size (32√ó32√ó3).

       ‚Ä¢	Mean pixel value: 121.04
     
	   ‚Ä¢	Pixel standard deviation: 64.39

2. **Data Preprocessing**
   
    1.	Normalization
    
	  ‚Ä¢	All images were normalized by dividing pixel values by 255.0, scaling them to the range [0, 1].

	  ‚Ä¢	Benefits: Improves convergence speed and stability during training.

	  ‚Ä¢	Post-normalization stats:

	    ‚Ä¢	Train images shape: (10,000, 32, 32, 3)
	    ‚Ä¢	Test images shape: (10,000, 32, 32, 3)
	    ‚Ä¢	Mean pixel value: 0.4747
	    ‚Ä¢	Pixel standard deviation: 0.2525

    2.	Label Preparation
       
	‚Ä¢	Flattened label arrays to ensure compatibility with sparse loss functions during training.

    ‚úÖ Outcome: Data is clean, scaled, and formatted correctly for CNN-based image classification.

3. **Baseline Model**
   
   **üîπ Pre-trained Model: ResNet50 Setup**

   We used ResNet50 (pre-trained on ImageNet) as the base model for CIFAR-10 classification, following these steps:

	 1.	Load Pre-trained ResNet50
 
	    ‚Ä¢	Imported ResNet50 with weights='imagenet', excluding the top classification layer (include_top=False) to use it as a feature extractor.
    	
	    ‚Ä¢	Input shape set to (32, 32, 3) for CIFAR-10 images.
	
	 2.	Freeze Base Layers
 
	    ‚Ä¢	Set base_model.trainable = False to retain the pre-trained weights and avoid updating them in the initial training phase.
	
	 3.	Add Custom Classification Head
 
	    ‚Ä¢	GlobalAveragePooling2D ‚Üí Converts feature maps into a single vector.
    	
	    ‚Ä¢	Dense(512, relu) ‚Üí First fully connected layer for feature learning.
    	
	    ‚Ä¢	Dense(256, relu) ‚Üí Second hidden layer for deeper representation.
    	
	    ‚Ä¢	Dense(10, softmax) ‚Üí Output layer for 10 CIFAR-10 classes.
	
	 4. Compile the Model
 
	    ‚Ä¢  Optimizer: Adam
   
	    ‚Ä¢  Loss: Sparse Categorical Crossentropy (for integer labels)
   
	    ‚Ä¢  Metric: Accuracy
	
	 5.	Train the Head
 
	    ‚Ä¢ Trained the custom head for 10 epochs with a batch size of 64, keeping the base model frozen.

    ‚úÖ Purpose: This approach leverages ResNet50‚Äôs powerful pre-trained features, while allowing the custom top layers to adapt specifically to CIFAR-10.
	
5. **Baseline Model evaluation**

   üìä Baseline Model Results (Frozen ResNet50 + Custom Head)

   **Training Performance:**
 
	‚Ä¢	Train Accuracy: 33.7%
	‚Ä¢	Validation Accuracy: 30.6%
	‚Ä¢	Train Loss: 1.8272
	‚Ä¢	Validation Loss: 1.9234

   **Progress Analysis:**
 
	‚Ä¢	‚úÖ Accuracy improved from ~13% (random guessing) to ~33% ‚Äî showing the model learned basic patterns.
	‚Ä¢	‚ö†Ô∏è Validation accuracy plateaued after epoch 4‚Äì5 (~30‚Äì32%), suggesting the head reached its learning limit.
	‚Ä¢	‚ö†Ô∏è Validation loss began increasing ‚Üí early signs of overfitting with frozen base layers.

   **Conclusion & Next Steps:**
 
	‚Ä¢	The frozen ResNet50 base provided useful features, but training only the custom head was insufficient.
	‚Ä¢	To improve:
	  1.	Unfreeze deeper ResNet50 layers for fine-tuning.
	  2.	Apply data augmentation to improve generalization.
	  3.	Train longer with early stopping to prevent overfitting.

	
6. **Fine-Tuning**
   
**üîß Fine-Tuning the Baseline ResNet50 Model**

Steps Taken

	1.	Unfrozen the Base Model
 
	    ‚Ä¢	Allowed deeper ResNet50 layers to be trainable so the model could adapt pretrained features to CIFAR-10.
	 
	2.	Added Two More Hidden Layers + Dropout
 
		‚Ä¢	Purpose of Hidden Layers:
		‚Ä¢	Learn more abstract patterns from ResNet50 output features.
		‚Ä¢	Improve feature interaction for complex class distinctions (e.g., cat vs dog).
		‚Ä¢	Purpose of Dropout:
		‚Ä¢	Reduce overfitting by randomly disabling neurons during training.
	    ‚Ä¢	Improve generalization to unseen data.
	 
	3.	Reduced Learning Rate
 
		‚Ä¢	Optimizer: Adam with learning_rate=1e-5 for stable transfer learning.
		‚Ä¢	Loss Function: sparse_categorical_crossentropy for multi-class classification.
		‚Ä¢	Metrics: Accuracy for training & validation monitoring.
  
	4.	Applied Data Augmentation + Early Stopping
 
		‚Ä¢	Data Augmentation Benefits:
		‚Ä¢	Expands training set via random transformations.
		‚Ä¢	Prevents overfitting & improves robustness to real-world variations.
		‚Ä¢	Early Stopping Benefits:
		‚Ä¢	Stops training when validation performance stops improving.
		‚Ä¢	Saves time & restores best model weights automatically.
  
	5.	Increased Epochs
 
		‚Ä¢	Trained for more epochs to allow deeper learning, balanced with early stopping to avoid overfitting.
  
7. **Evaluation after fine-tuning**

After fine-tuning the ResNet50 model with additional hidden layers, dropout regularization, reduced learning rate, data augmentation, and early stopping, the model achieved 71.04% accuracy on the test set.
 
**üìä Model Evaluation & Results**

 **Training vs Validation Accuracy**
 
![Training vs Validation Accuracy](assets/train_val_accuracy.png)

	‚Ä¢	Observation: Accuracy steadily improved for both training and validation sets.
	‚Ä¢	Key Insight: The gap between training and validation accuracy is small, indicating reduced overfitting due to dropout and data augmentation.
	‚Ä¢	Peak Validation Accuracy: ~71%

**Training vs Validation Loss**

![Training vs Validation Loss](assets/train_val_loss.png)

	‚Ä¢	Observation: Loss decreases steadily without significant divergence between training and validation curves.
	‚Ä¢	Key Insight: Model generalized well and avoided severe overfitting.

**Confusion Matrix**

![Confusion Matrix](assets/confusion_matrix.png)

	‚Ä¢	Observation:
	‚Ä¢	High accuracy for frog (class 6), ship (class 8), and automobile (class 1).
	‚Ä¢	Lower performance for cat (class 3) and bird (class 2) ‚Äî these may share visual similarities with other classes (e.g., dog, deer).
	‚Ä¢	Key Insight: Misclassifications often occur between visually similar classes.

**Classification Report**

![Classification Report](assets/classification_report.png)

	‚Ä¢	Macro Avg F1-Score: 0.704
	‚Ä¢	Highest Precision: Ship (0.8828)
	‚Ä¢	Highest Recall: Frog (0.8920)
	‚Ä¢	Lowest F1: Cat (0.4988) ‚Üí needs targeted improvement.

**Metrics by Class**

![Metrics by Class](assets/metrics_by_class.png)

	‚Ä¢	Observation: Most classes maintain balanced precision and recall.
	‚Ä¢	Insight: Variations between precision and recall highlight the trade-off between false positives and false negatives per class.
 
![Metrics by Class](assets/all_metric.png)


‚úÖ Final Summary

	‚Ä¢	Final Test Accuracy: 71.04%
	‚Ä¢	Strengths: Strong performance on structured, distinctive classes like automobiles, ships, and frogs.
	‚Ä¢	Weaknesses: Struggles with classes that have high intra-class variation (cats, birds).
	‚Ä¢	Future Work:
	‚Ä¢	Class-specific data augmentation (e.g., more varied cat/bird images).
	‚Ä¢	Fine-tuning additional layers in ResNet50.
	‚Ä¢	Experimenting with learning rate schedules.

8. **Evaluation after further fine-tuning**

**üìå Updated CIFAR-10 Image Classification Model (88.82% Accuracy)**

**üöÄ Update Summary**

In this updated version of the CIFAR-10 classification project, two major improvements were implemented based on previous analysis and suggestions:

1. **Resize Images to 96√ó96**
   
   - The original dataset consists of 32√ó32 images.
   - Resizing to 96√ó96 allows the pre-trained ResNet50 model to capture richer spatial and texture features.
   - This step aligns better with the model's original training resolution, boosting feature extraction quality.

3. **Balanced Class-Weighted Loss**
   
   - Applied `class_weight` parameter during training to handle any class imbalance.
   - Helps the model give fair importance to underrepresented classes.
   - Reduces bias towards dominant classes and improves macro average metrics.

---

**üìä Results After Improvements**

- **Final Accuracy:** **88.82%** ‚úÖ

  ![Train_Validation_Accuracy](assets/2nd_train_val_accuracy.png)
  ![Train_Validation_loss](assets/2nd_train_val_loss.png)
  ![Classification report](assets/2nd_classification.png)
  
- Significant boost from previous fine-tuned accuracy (**71.04%**).
- Noticeable improvement across all classes in **Precision, Recall, and F1-score**.
- Reduced misclassification between visually similar classes (e.g., cat vs dog, automobile vs truck).

---

## üìÇ Colab Notebook

You can run the updated version in Google Colab here:
This part is only the base model with the unfreeze option.

üîó **[Open in Colab]([YOUR_COLAB_LINK_HERE](https://colab.research.google.com/drive/1ihXbcwJw1KsqEhGQY5ChkOigSSfGl5Lw?usp=sharing))**

---

**üîç Key Benefits of These Changes**

- **Higher Resolution Input** ‚Üí More detailed feature maps, better object boundaries detected.
- **Class-Weighted Loss** ‚Üí Improved fairness in classification results, higher macro F1-score.
- **Better Generalization** ‚Üí Model performs well on both seen and unseen data.

---

**üìå Next Steps**

- Experiment with **EfficientNetB3** for even better performance with 96√ó96 input.
- Use **mixup** or **cutmix** augmentation to further improve robustness.
- Try **learning rate scheduling** for optimized convergence.

---
---

## üöÄ Usage

### 1Ô∏è‚É£ Run in Google Colab
Click below to run the notebook directly in Colab:  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YourGitHubUser/Image-Classification-CIFAR10-ResNet50/blob/main/ImageClassification_CIFAR10_CV_Project.ipynb)

---

### 2Ô∏è‚É£ Run Locally
```bash
git clone https://github.com/YourGitHubUser/Image-Classification-CIFAR10-ResNet50.git
cd Image-Classification-CIFAR10-ResNet50
pip install -r requirements.txt
jupyter notebook ImageClassification_CIFAR10_CV_Project.ipynb
