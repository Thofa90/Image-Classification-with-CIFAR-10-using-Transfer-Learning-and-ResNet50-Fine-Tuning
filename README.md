# üñºÔ∏è Image Classification with CIFAR-10 using Transfer Learning (ResNet50)

## üìå Project Overview

This project focuses on building a deep learning-based **image classifier** for the CIFAR-10 dataset using **transfer learning** with **ResNet50** pre-trained model.  
We start with a **baseline model** (31.76% accuracy) and progressively fine-tune it using:
- Unfreezing ResNet50 layers
- Adding custom dense layers with dropout
- Lowering the learning rate
- Applying data augmentation
- Using early stopping to prevent overfitting

The final model achieved **88.82% accuracy** on the test set.

---

## üéØ Goal

To develop a **highly accurate, generalizable** image classification model by leveraging **transfer learning** and advanced training techniques, making it applicable to real-world scenarios such as product categorization, surveillance, and autonomous systems.

---

## üíº Business Context

Image classification is critical in:
- **Retail** (automatic product tagging)
- **Automotive** (object detection in self-driving cars)
- **Healthcare** (medical imaging classification)
- **Security** (identifying threats in real time)

By applying deep learning with transfer learning, companies can leverage pre-trained models on massive datasets and adapt them to their own classification tasks without starting from scratch ‚Äî saving both time and computational resources.

---

## üåç Real-World Applications

- **E-commerce**: Categorizing products from images automatically.
- **Autonomous Vehicles**: Detecting traffic objects and hazards.
- **Medical Imaging**: Classifying medical scans for diagnostics.
- **Social Media**: Filtering inappropriate image content.

---

## üìÇ Dataset

The project uses the **CIFAR-10** dataset:
This **dataset is also available** in TensorFlow & Keras. 
Here is how you can import it: from tensorflow.keras.datasets import cifar10
- **60,000** 32x32 RGB images
- **10 classes**, each with **6,000 images**
- **50,000 training** images and **10,000 test** images

**Classes:** airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.

---

## üìä Project Workflow

1. **EDA Summary**
   
   **- Dataset shape & class distribution**
     
       ‚Ä¢    Training set: 10,000 RGB images, size 32√ó32√ó3
     
	   ‚Ä¢	Test set: 10,000 RGB images, size 32√ó32√ó3

	   ‚Ä¢	Classes: 10 mutually exclusive categories (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)

       ‚Ä¢	Balanced across categories, ranging from 937 to 1,032 images per class.
       
   **- Sample image visualization**
   **- Pixel intensity analysis**
     
     	‚Ä¢	Most pixel values range from 50 to 150 (moderate brightness).
     
	    ‚Ä¢	Few pixels are near 0 (pure black) or 255 (pure white).
     
	    ‚Ä¢	Indicates well-lit, balanced images without extreme contrast.
     
        ‚Ä¢	Red & Green channels peak around 100‚Äì120, fairly symmetric.
     
	    ‚Ä¢	Blue channel slightly darker overall, peaking near 100.
     
	    ‚Ä¢	No strong color bias ‚Äî suitable for CNN training.
     
   **- Data Quality and Basic Statistics**
     
       ‚Ä¢	No missing, NaN, or Inf values.
     
	   ‚Ä¢	All images have consistent size (32√ó32√ó3).

       ‚Ä¢	Mean pixel value: 121.04
     
	   ‚Ä¢	Pixel standard deviation: 64.39

2. **Data Preprocessing**
   
    1.	Normalization
    
	  ‚Ä¢	All images were normalized by dividing pixel values by 255.0, scaling them to the range [0, 1].

	  ‚Ä¢	Benefits: Improves convergence speed and stability during training.

	  ‚Ä¢	Post-normalization stats:

	    ‚Ä¢	Train images shape: (10,000, 32, 32, 3)
	    ‚Ä¢	Test images shape: (10,000, 32, 32, 3)
	    ‚Ä¢	Mean pixel value: 0.4747
	    ‚Ä¢	Pixel standard deviation: 0.2525

    2.	Label Preparation
       
	‚Ä¢	Flattened label arrays to ensure compatibility with sparse loss functions during training.

    ‚úÖ Outcome: Data is clean, scaled, and formatted correctly for CNN-based image classification.

3. **Baseline Model**
   
**üîπ Pre-trained Model: ResNet50 Setup**

We used ResNet50 (pre-trained on ImageNet) as the base model for CIFAR-10 classification, following these steps:

	1.	Load Pre-trained ResNet50
 
	   ‚Ä¢	Imported ResNet50 with weights='imagenet', excluding the top classification layer (include_top=False) to use it as a feature extractor.
	   ‚Ä¢	Input shape set to (32, 32, 3) for CIFAR-10 images.
	
	2.	Freeze Base Layers
 
	   ‚Ä¢	Set base_model.trainable = False to retain the pre-trained weights and avoid updating them in the initial training phase.
	
	3.	Add Custom Classification Head
 
	   ‚Ä¢	GlobalAveragePooling2D ‚Üí Converts feature maps into a single vector.
	   ‚Ä¢	Dense(512, relu) ‚Üí First fully connected layer for feature learning.
	   ‚Ä¢	Dense(256, relu) ‚Üí Second hidden layer for deeper representation.
	   ‚Ä¢	Dense(10, softmax) ‚Üí Output layer for 10 CIFAR-10 classes.
	
	4.	Compile the Model
 
	   ‚Ä¢	Optimizer: Adam
	   ‚Ä¢	Loss: Sparse Categorical Crossentropy (for integer labels)
	   ‚Ä¢	Metric: Accuracy
	
	5.	Train the Head
 
	   ‚Ä¢	Trained the custom head for 10 epochs with a batch size of 64, keeping the base model frozen.

    ‚úÖ Purpose: This approach leverages ResNet50‚Äôs powerful pre-trained features, while allowing the custom top layers to adapt specifically to CIFAR-10.
	
4. **Baseline Model evaluation**

üìä Baseline Model Results (Frozen ResNet50 + Custom Head)

**Training Performance:**
	‚Ä¢	Train Accuracy: 33.7%
	‚Ä¢	Validation Accuracy: 30.6%
	‚Ä¢	Train Loss: 1.8272
	‚Ä¢	Validation Loss: 1.9234

**Progress Analysis:**
	‚Ä¢	‚úÖ Accuracy improved from ~13% (random guessing) to ~33% ‚Äî showing the model learned basic patterns.
	‚Ä¢	‚ö†Ô∏è Validation accuracy plateaued after epoch 4‚Äì5 (~30‚Äì32%), suggesting the head reached its learning limit.
	‚Ä¢	‚ö†Ô∏è Validation loss began increasing ‚Üí early signs of overfitting with frozen base layers.

**Conclusion & Next Steps:**
	‚Ä¢	The frozen ResNet50 base provided useful features, but training only the custom head was insufficient.
	‚Ä¢	To improve:
	  1.	Unfreeze deeper ResNet50 layers for fine-tuning.
	  2.	Apply data augmentation to improve generalization.
	  3.	Train longer with early stopping to prevent overfitting.

	
5. **Fine-Tuning**
   - Unfreezing layers
   - Adding more hidden layers & dropout
   - Reducing learning rate
   - Applying early stopping
6. **Evaluation**
   - Accuracy/Loss curves
   - Confusion matrix
   - Classification report

---

## üöÄ Usage

### 1Ô∏è‚É£ Run in Google Colab
Click below to run the notebook directly in Colab:  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YourGitHubUser/Image-Classification-CIFAR10-ResNet50/blob/main/ImageClassification_CIFAR10_CV_Project.ipynb)

---

### 2Ô∏è‚É£ Run Locally
```bash
git clone https://github.com/YourGitHubUser/Image-Classification-CIFAR10-ResNet50.git
cd Image-Classification-CIFAR10-ResNet50
pip install -r requirements.txt
jupyter notebook ImageClassification_CIFAR10_CV_Project.ipynb
